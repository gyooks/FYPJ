# -*- coding: utf-8 -*-
"""FYP Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XC-77GeK8sNTs2lrmKkxPRB8JPp2q3wv
"""

!pip install tensorflow opencv-python-headless numpy scikit-learn

from google.colab import drive
drive.mount('/content/drive')

import os
import glob
import time

# Define the Drive location where the dataset is already stored
extracted_dir = '/content/drive/My Drive/fyp dataset egogesture'

# Verify the directory exists
if os.path.exists(extracted_dir):
    print(f"Extracted directory found: {extracted_dir}")
    # Force refresh of directory listing
    time.sleep(2)  # Delay to ensure sync
    print(f"Contents of {extracted_dir}: {os.listdir(extracted_dir)}")

    # Verify videos folder
    videos_dir = os.path.join(extracted_dir, 'videos')
    if os.path.exists(videos_dir):
        video_files = glob.glob(os.path.join(videos_dir, '*'))
        print(f"Videos folder contains {len(video_files)} items (subfolders/files):")
        for file in video_files[:5]:  # Show first 5 for brevity
            print(f"  - {os.path.basename(file)}")
        if len(video_files) > 5:
            print(f"  - ... and {len(video_files) - 5} more items")
    else:
        print("Videos folder not found.")

    # Verify labels folder with recursive search
    labels_dir = os.path.join(extracted_dir, 'labels')
    print(f"Checking labels directory: {labels_dir}")
    if os.path.exists(labels_dir):
        print(f"Labels directory exists. Contents: {os.listdir(labels_dir)}")
        # Check all files in labels and subdirectories
        all_label_files = glob.glob(os.path.join(labels_dir, '**'), recursive=True)
        print(f"All files/folders in labels (including subdirectories): {len(all_label_files)}")
        for file in all_label_files[:5]:  # Show first 5 for brevity
            print(f"  - {os.path.basename(file)} (full path: {file})")
        if len(all_label_files) > 5:
            print(f"  - ... and {len(all_label_files) - 5} more items")
        # Check for .csv files recursively
        label_csv_files = glob.glob(os.path.join(labels_dir, '**', '*.csv'), recursive=True)
        print(f"Total .csv files in labels: {len(label_csv_files)}")
        for file in label_csv_files[:5]:  # Show first 5 for brevity
            print(f"  - {os.path.basename(file)} (full path: {file})")
        if len(label_csv_files) > 5:
            print(f"  - ... and {len(label_csv_files) - 5} more files")
        if len(label_csv_files) != 8:  # Adjust based on expected number
            print(f"Warning: Expected 8 .csv files based on screenshot, but found {len(label_csv_files)}.")
    else:
        print("Labels folder not found. Directory does not exist.")
else:
    print("Extracted directory not found. Ensure the 'fyp dataset egogesture' folder is correctly set up in Google Drive and Drive is mounted.")

import pandas as pd
import glob
import os

annotations_dir = '/content/drive/My Drive/fyp dataset egogesture/labels'
videos_dir = '/content/drive/My Drive/fyp dataset egogesture/videos'
desired_labels = [10, 11, 57, 58, 46, 36]

# Minimal directory checks
print(f"Checking directories...")
if not os.path.exists(annotations_dir):
    print(f"Error: Annotations directory {annotations_dir} does not exist.")
if not os.path.exists(videos_dir):
    print(f"Error: Videos directory {videos_dir} does not exist.")

all_annotations = []
for ann_file in glob.glob(os.path.join(annotations_dir, '**', '*.csv'), recursive=True):
    try:
        sample = pd.read_csv(ann_file, nrows=1, sep=',')
        header = 0
        if not sample.iloc[0].astype(str).str.match(r'^\\d+(\\.\\d+)?$').all():
            header = None

        annotations = pd.read_csv(ann_file, sep=',', names=['label', 'start_frame', 'end_frame'], header=header)
        annotations['label'] = pd.to_numeric(annotations['label'], errors='coerce')
        annotations = annotations.dropna(subset=['label'])
        annotations['start_frame'] = pd.to_numeric(annotations['start_frame'], errors='coerce')
        annotations['end_frame'] = pd.to_numeric(annotations['end_frame'], errors='coerce')
        annotations = annotations.dropna(subset=['start_frame', 'end_frame'])

        filtered = annotations[annotations['label'].isin(desired_labels)]
        if not filtered.empty:
            video_base = os.path.basename(ann_file).replace('Group', 'rgb').replace('.csv', '.avi')
            all_annotations.append(filtered.assign(video=video_base))
    except Exception as e:
        print(f"Error loading {ann_file}: {e}")

if all_annotations:
    all_annotations = pd.concat(all_annotations, ignore_index=True)
    print(f"Cell executed successfully. Total combined annotations: {len(all_annotations)}")
else:
    print("Cell executed, but no annotations loaded.")

import cv2
import numpy as np
import os
import glob
import pandas as pd

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define paths
extracted_dir = '/content/drive/My Drive/fyp dataset egogesture'
annotations_dir = os.path.join(extracted_dir, 'labels')
videos_dir = os.path.join(extracted_dir, 'videos')
feature_save_dir = extracted_dir

# Load and filter annotations
desired_labels = [10, 11, 57, 58, 46, 36]
all_annotations = []

print(f"Checking annotations directory: {annotations_dir}")
if os.path.exists(annotations_dir):
    print(f"Annotations directory exists. Contents: {os.listdir(annotations_dir)}")
else:
    print(f"Error: Annotations directory {annotations_dir} does not exist.")

print(f"Checking videos directory: {videos_dir}")
if os.path.exists(videos_dir):
    print(f"Videos directory exists. Contents: {os.listdir(videos_dir)}")
else:
    print(f"Error: Videos directory {videos_dir} does not exist.")

for ann_file in glob.glob(os.path.join(annotations_dir, '**', '*.csv'), recursive=True):
    print(f"Processing file: {ann_file}")
    try:
        sample = pd.read_csv(ann_file, nrows=1, sep=',')
        header = 0 if not sample.iloc[0].astype(str).str.match(r'^\d+(\.\d+)?$').all() else None
        annotations = pd.read_csv(ann_file, sep=',', names=['label', 'start_frame', 'end_frame'], header=header)

        annotations['label'] = pd.to_numeric(annotations['label'], errors='coerce')
        annotations = annotations.dropna(subset=['label'])
        annotations['start_frame'] = pd.to_numeric(annotations['start_frame'], errors='coerce')
        annotations['end_frame'] = pd.to_numeric(annotations['end_frame'], errors='coerce')
        annotations = annotations.dropna(subset=['start_frame', 'end_frame'])

        filtered = annotations[annotations['label'].isin(desired_labels)]
        if not filtered.empty:
            print(f"Filtered annotations from {ann_file}:")
            print(filtered.head())
            # Adjust video base to match case and structure
            video_base = os.path.basename(ann_file).replace('Group', 'rgb').replace('.csv', '.avi')
            subject_dir = os.path.basename(os.path.dirname(os.path.dirname(ann_file))).replace('subject', 'Subject')
            scene_dir = os.path.basename(os.path.dirname(ann_file))
            video_path = None
            for subdir in ['videos_1-001', 'videos_2-002', 'videos_3-004', 'videos_4-003', 'videos_5-003']:
                check_path = os.path.join(videos_dir, subdir, subject_dir, scene_dir, 'Color', video_base)
                if os.path.exists(check_path):
                    video_path = check_path
                    break
            if video_path is None:
                print(f"Error: Video file not found for {video_base} in any subdirectory. Skipping.")
                continue
            print(f"Assigning video path: {video_path}")
            all_annotations.append(filtered.assign(video=video_path))
        else:
            print(f"No matching labels found in {ann_file} from desired_labels: {desired_labels}")
    except Exception as e:
        print(f"Error loading or processing {ann_file}: {e}")

if all_annotations:
    all_annotations = pd.concat(all_annotations, ignore_index=True)
    print(f"\nTotal combined annotations: {len(all_annotations)}")
    print("Final combined annotations:")
    print(all_annotations.head())
else:
    all_annotations = pd.DataFrame(columns=['label', 'start_frame', 'end_frame', 'video'])
    print("\nNo annotations were loaded or filtered successfully from any file.")
    print("Final combined annotations is an empty DataFrame.")

def extract_sequences(annotations, videos_dir):
    sequences = []
    for index, row in annotations.iterrows():
        video_path = row['video']
        if not os.path.exists(video_path):
            print(f"Error: Video file not found at {video_path}. Skipping.")
            continue
        print(f"Attempting to open: {video_path}")
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"Error: Could not open video {video_path}")
            continue
        start_frame = int(row['start_frame'])
        end_frame = int(row['end_frame'])
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

        frames = []
        frame_count = 0
        target_frames = 30

        while frame_count < target_frames and cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                print(f"Warning: Reached end of video at frame {start_frame + frame_count} in {video_path}")
                break
            if start_frame + frame_count <= end_frame:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB
                frames.append(cv2.resize(frame_rgb, (64, 64)))     # Resize to 64x64
                frame_count += 1
            else:
                print(f"Warning: Reached end_frame {end_frame} at frame {start_frame + frame_count} in {video_path}")
                break

        cap.release()

        # Pad or truncate to exactly 30 frames
        if len(frames) < target_frames:
            print(f"Warning: Only {len(frames)} frames extracted from {video_path}. Padding with last frame.")
            last_frame = frames[-1] if frames else np.zeros((64, 64, 3), dtype=np.uint8)
            frames = frames + [last_frame] * (target_frames - len(frames))
        elif len(frames) > target_frames:
            print(f"Warning: Extracted {len(frames)} frames from {video_path}. Truncating to {target_frames}.")
            frames = frames[:target_frames]

        sequences.append(np.array(frames))

    return sequences

# Main execution block
if 'all_annotations' in globals() and not all_annotations.empty:
    print(f"Extracting sequences from {len(all_annotations)} annotations...")
    sequences = extract_sequences(all_annotations, videos_dir)
    print(f"Extracted {len(sequences)} sequences of 30 frames each.")
    # Save the extracted sequences
    os.makedirs(feature_save_dir, exist_ok=True)
    np.save(os.path.join(feature_save_dir, 'all_features.npy'), np.array(sequences))
    print(f"Features saved to: {os.path.join(feature_save_dir, 'all_features.npy')}")
else:
    print("Error: all_annotations is not defined or empty.")

def extract_optical_flow_features(sequence):
    flow_features = []
    for i in range(len(sequence) - 1):
        frame1 = cv2.cvtColor(sequence[i], cv2.COLOR_BGR2GRAY)
        frame2 = cv2.cvtColor(sequence[i + 1], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(frame1, frame2, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_features.append(np.array([np.mean(magnitude), np.mean(angle)]))
    return np.array(flow_features)

if 'sequences' in globals() and sequences:
    print(f"Extracting features from {len(sequences)} sequences...")
    # Verify and adjust sequences to ensure 30 frames
    all_features = []
    for i, seq in enumerate(sequences):
        if len(seq) != 30:
            print(f"Warning: Sequence {i} has {len(seq)} frames instead of 30. Padding with last frame.")
            last_frame = seq[-1] if seq else np.zeros((64, 64, 3), dtype=np.uint8)
            seq = np.concatenate([seq, [last_frame] * (30 - len(seq))]) if len(seq) < 30 else seq[:30]
        all_features.append(seq)
    all_features = np.array(all_features)
    # Save to feature_save_dir
    save_path = os.path.join(feature_save_dir, 'all_features.npy')
    np.save(save_path, all_features)
    print(f"Total features shape: {all_features.shape}")
    print(f"Features saved as {save_path}.")
else:
    print("Error: No sequences extracted.")

print("Class distribution:", all_annotations['label'].value_counts())

import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, TimeDistributed, GlobalAveragePooling2D, Bidirectional, LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers.schedules import ExponentialDecay
import glob
import os
from sklearn.utils import class_weight
import tensorflow as tf
import albumentations as A

# Enable mixed precision
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# Define the directory where features were saved
feature_save_dir = '/content/drive/My Drive/fyp dataset egogesture'

# Load features from the Drive location
print(f"Attempting to load feature files from: {feature_save_dir}")
feature_files = glob.glob(os.path.join(feature_save_dir, 'all_features.npy'))

print(f"Found {len(feature_files)} feature files.")
if not feature_files:
    print("Error: No feature files found. Please ensure the previous extraction and saving step completed successfully.")
    all_features = []
else:
    all_features = []
    for file in feature_files:
        try:
            batch_data = np.load(file, allow_pickle=True)
            all_features.append(batch_data)
            print(f"Loaded {file} with shape {batch_data.shape}.")
            if batch_data.shape[1] != 30:
                print(f"Warning: Expected 30 frames, got {batch_data.shape[1]} in {file}.")
        except Exception as e:
            print(f"Error loading {file}: {e}")

if all_features:
    try:
        all_features = np.concatenate(all_features) if len(all_features) > 1 else all_features[0]
        print(f"Successfully loaded and concatenated {len(all_features)} feature sequences.")
        print(f"Shape of consolidated features: {all_features.shape}")

        if 'all_annotations' in globals() and all_features.size > 0 and not all_annotations.empty:
            if len(all_features) != len(all_annotations):
                print(f"Warning: Number of extracted feature sequences ({len(all_features)}) does not match the number of annotations ({len(all_annotations)}).")
                min_len = min(len(all_features), len(all_annotations))
                all_features = all_features[:min_len]
                all_annotations = all_annotations.iloc[:min_len]
                print(f"Truncating features and annotations to match length: {min_len}")

            X = all_features
            y = all_annotations['label'].values

            unique_labels = np.unique(y)
            label_map = {label: i for i, label in enumerate(unique_labels)}
            y_mapped = np.array([label_map[label] for label in y])
            y_categorical = to_categorical(y_mapped, num_classes=len(unique_labels))

            class_weights = class_weight.compute_class_weight('balanced', classes=unique_labels, y=y)
            class_weight_dict = dict(enumerate(class_weights))

            print(f"Shape of X before split: {X.shape}")
            print(f"Shape of y_categorical before split: {y_categorical.shape}")
            print(f"Class weights: {class_weight_dict}")
            print("Class distribution:", np.bincount(y_mapped))
            print("NaN in X:", np.isnan(X).any())
            print("Inf in X:", np.isinf(X).any())

            if X.shape[0] < 2 or y_categorical.shape[0] < 2:
                print("Error: Not enough samples to perform train/test split.")
            else:
                X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42, stratify=y_mapped)

                X_train = X_train.astype('float32') / 255.0
                X_test = X_test.astype('float32') / 255.0

                batch_size = 8
                steps_per_epoch = len(X_train) // batch_size
                validation_steps = (len(X_test) + batch_size - 1) // batch_size

                print(f"Shape of X_train: {X_train.shape}")
                print(f"Shape of X_test: {X_test.shape}")
                print(f"Shape of y_train: {y_train.shape}")
                print(f"Shape of y_test: {y_test.shape}")

                expected_input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3], X_train.shape[4])
                print(f"Expected input shape: {expected_input_shape}")
                if X_train.shape[1] != 30 or X_train.shape[2:] != (64, 64, 3):
                    print(f"Warning: Unexpected feature shape {X_train.shape}. Expected (samples, 30, 64, 64, 3).")

                # Data augmentation
                transform = A.Compose([
                    A.HorizontalFlip(p=0.5),
                    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),
                    A.Rotate(limit=10, p=0.5),
                    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.5)
                ])
                def augment_sequence(seq):
                    aug_seq = []
                    for frame in seq:
                        augmented = transform(image=frame)['image']
                        aug_seq.append(augmented)
                    return np.array(aug_seq)

                # Data generator with class balancing
                def data_generator(X, y, batch_size, augment=True):
                    weights = np.array([class_weight_dict[np.argmax(y[i])] for i in range(len(y))])
                    weights /= weights.sum()
                    while True:
                        indices = np.random.choice(len(X), size=len(X), p=weights)
                        for start_idx in range(0, len(X), batch_size):
                            end_idx = min(start_idx + batch_size, len(X))
                            batch_indices = indices[start_idx:end_idx]
                            X_batch = X[batch_indices]
                            y_batch = y[batch_indices]
                            if augment:
                                X_batch_aug = []
                                y_batch_aug = []
                                for i in range(len(X_batch)):
                                    X_batch_aug.append(X_batch[i])
                                    y_batch_aug.append(y_batch[i])
                                    for _ in range(2):
                                        aug_seq = augment_sequence(X_batch[i])
                                        X_batch_aug.append(aug_seq)
                                        y_batch_aug.append(y_batch[i])
                                X_batch = np.array(X_batch_aug)
                                y_batch = np.array(y_batch_aug)
                            yield X_batch.astype('float32'), y_batch

                # Define model with pre-trained MobileNetV2 and fine-tuning
                base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(64, 64, 3))
                base_model.trainable = True  # Fine-tune all layers
                for layer in base_model.layers[:-10]:  # Freeze all but last 10 layers
                    layer.trainable = False
                inputs = Input(shape=(30, 64, 64, 3))
                x = TimeDistributed(base_model)(inputs)
                x = TimeDistributed(GlobalAveragePooling2D())(x)
                x = Bidirectional(LSTM(256))(x)
                x = BatchNormalization()(x)
                x = Dropout(0.4)(x)  # Increased dropout
                x = Dense(128, activation='relu', kernel_regularizer='l2')(x)
                x = BatchNormalization()(x)
                x = Dropout(0.4)(x)  # Increased dropout
                outputs = Dense(len(unique_labels), activation='softmax')(x)

                model = Model(inputs=inputs, outputs=outputs)

                # Learning rate schedule
                lr_schedule = ExponentialDecay(initial_learning_rate=0.0005, decay_steps=1000, decay_rate=0.9)
                optimizer = Adam(learning_rate=lr_schedule)
                model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

                # Callback
                reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)

                model.summary()
                !nvidia-smi

                # Train model with generator
                train_generator = data_generator(X_train, y_train, batch_size, augment=True)
                val_generator = data_generator(X_test, y_test, batch_size, augment=False)
                history = model.fit(train_generator, epochs=100, steps_per_epoch=steps_per_epoch,
                                   validation_data=val_generator, validation_steps=validation_steps,
                                   verbose=2, callbacks=[reduce_lr])

                # Print training history
                for epoch in range(len(history.history['accuracy'])):
                    print(f"Epoch {epoch+1}: Train Accuracy = {history.history['accuracy'][epoch]:.4f}, Val Accuracy = {history.history['val_accuracy'][epoch]:.4f}, Train Loss = {history.history['loss'][epoch]:.4f}, Val Loss = {history.history['val_loss'][epoch]:.4f}")

                # Evaluate model
                test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
                print(f"Test accuracy: {test_accuracy:.4f}")
                print(f"Test loss: {test_loss:.4f}")
                print(f"Best validation loss: {min(history.history['val_loss'])}")

                # Confusion matrix and saving
                from sklearn.metrics import confusion_matrix
                y_pred = model.predict(X_test)
                cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))
                print("Confusion Matrix:\n", cm)

                os.makedirs(feature_save_dir, exist_ok=True)
                model_save_path = os.path.join(feature_save_dir, 'gesture_recognition_model.keras')
                label_map_save_path = os.path.join(feature_save_dir, 'label_map.npy')
                unique_labels_save_path = os.path.join(feature_save_dir, 'unique_labels.npy')
                model.save(model_save_path)
                np.save(label_map_save_path, label_map)
                np.save(unique_labels_save_path, unique_labels)
                print(f"Model saved to: {model_save_path}")
                print(f"Label map saved to: {label_map_save_path}")
                print(f"Unique labels saved to: {unique_labels_save_path}")
                print("Model and metadata saved.")
        else:
            print("Error: Data not available for training (all_annotations empty or no features loaded).")
    except ValueError as ve:
        print(f"ValueError during concatenation or processing: {ve}")
    except Exception as e:
        print(f"An unexpected error occurred during model training: {e}")
else:
    print("Error: No features loaded from files. Cannot proceed with training.")

from sklearn.metrics import confusion_matrix
y_pred = model.predict(X_test)
cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))
print("Confusion Matrix:\n", cm)

import os
print(os.listdir('/content/'))

from google.colab import files
files.download('gesture_recognition_model.h5')
files.download('label_map.npy')
files.download('unique_labels.npy')

import psutil
print(psutil.virtual_memory())